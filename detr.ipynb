{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6091d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RT-DETR model and processor...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from /home/ai-user/Paligemma/bbox/newthous...\n",
      "Found 963 images for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/963 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image: /home/ai-user/Paligemma/bbox/newthous/Normal/2025_03_12_24440223_7A1E03C6_EEEF769F.jpeg\n",
      "Raw model outputs:\n",
      "  Logits shape: torch.Size([1, 300, 2])\n",
      "  Logits: [[[-7.202348  -9.439762 ]\n",
      "  [-6.5972433 -8.634392 ]\n",
      "  [-7.718758  -9.268696 ]\n",
      "  [-7.577014  -8.998146 ]\n",
      "  [-7.6719265 -9.099407 ]\n",
      "  [-7.1135993 -9.308796 ]\n",
      "  [-7.1048274 -8.837274 ]\n",
      "  [-6.3116074 -8.679416 ]\n",
      "  [-7.1448526 -8.763713 ]\n",
      "  [-7.621596  -9.164952 ]\n",
      "  [-7.7380857 -9.502893 ]\n",
      "  [-7.3225927 -9.0370035]\n",
      "  [-7.359062  -9.128228 ]\n",
      "  [-7.223075  -8.760273 ]\n",
      "  [-7.408363  -9.160225 ]\n",
      "  [-6.369876  -9.673777 ]\n",
      "  [-7.462728  -9.202681 ]\n",
      "  [-6.5280967 -8.646177 ]\n",
      "  [-6.9761596 -9.155717 ]\n",
      "  [-7.1505723 -8.697798 ]\n",
      "  [-7.2662454 -9.3257885]\n",
      "  [-7.3927655 -8.971735 ]\n",
      "  [-7.419917  -9.10144  ]\n",
      "  [-7.373649  -8.871774 ]\n",
      "  [-7.147759  -9.027254 ]\n",
      "  [-7.028318  -9.356888 ]\n",
      "  [-7.180586  -9.259307 ]\n",
      "  [-7.067431  -9.237716 ]\n",
      "  [-7.3736925 -9.034832 ]\n",
      "  [-6.819785  -8.679187 ]\n",
      "  [-7.0726027 -8.850122 ]\n",
      "  [-6.738962  -9.127318 ]\n",
      "  [-7.1351404 -9.1376915]\n",
      "  [-7.101753  -9.112999 ]\n",
      "  [-7.5581975 -9.333832 ]\n",
      "  [-7.763709  -9.329593 ]\n",
      "  [-6.866269  -9.022104 ]\n",
      "  [-6.8294826 -9.140837 ]\n",
      "  [-7.82254   -9.136243 ]\n",
      "  [-7.26465   -9.122911 ]\n",
      "  [-6.669822  -8.667838 ]\n",
      "  [-7.013992  -8.596101 ]\n",
      "  [-7.1948195 -9.054829 ]\n",
      "  [-7.9146314 -9.363797 ]\n",
      "  [-7.6003127 -9.601181 ]\n",
      "  [-7.4503613 -8.787144 ]\n",
      "  [-6.2650285 -8.321441 ]\n",
      "  [-7.3154864 -9.155159 ]\n",
      "  [-7.6700883 -9.4442425]\n",
      "  [-7.5595775 -9.673649 ]\n",
      "  [-7.027371  -8.776819 ]\n",
      "  [-7.5287685 -9.225645 ]\n",
      "  [-7.7000175 -9.254763 ]\n",
      "  [-7.06237   -8.945932 ]\n",
      "  [-7.2002416 -9.221016 ]\n",
      "  [-6.819431  -8.977452 ]\n",
      "  [-6.9457393 -8.846296 ]\n",
      "  [-7.5131345 -8.831112 ]\n",
      "  [-7.6902723 -9.704731 ]\n",
      "  [-7.076678  -9.127517 ]\n",
      "  [-6.851891  -8.58159  ]\n",
      "  [-7.700819  -9.377454 ]\n",
      "  [-7.649087  -9.198542 ]\n",
      "  [-6.831763  -9.182907 ]\n",
      "  [-6.363952  -9.162033 ]\n",
      "  [-7.084245  -9.029621 ]\n",
      "  [-7.3069315 -9.24139  ]\n",
      "  [-7.1705337 -8.861048 ]\n",
      "  [-8.296013  -9.30524  ]\n",
      "  [-7.352719  -9.502682 ]\n",
      "  [-7.4333405 -8.895567 ]\n",
      "  [-7.190883  -8.93457  ]\n",
      "  [-7.794521  -9.183857 ]\n",
      "  [-6.940603  -9.10844  ]\n",
      "  [-7.3375278 -8.94003  ]\n",
      "  [-7.53031   -9.079054 ]\n",
      "  [-7.024705  -8.898542 ]\n",
      "  [-6.9025626 -9.039581 ]\n",
      "  [-7.1863747 -9.036135 ]\n",
      "  [-6.293175  -8.529794 ]\n",
      "  [-7.402555  -9.031318 ]\n",
      "  [-7.2914634 -9.426197 ]\n",
      "  [-6.127352  -8.468991 ]\n",
      "  [-7.495765  -9.21673  ]\n",
      "  [-7.907715  -9.299342 ]\n",
      "  [-6.761317  -8.58373  ]\n",
      "  [-7.2158165 -9.108279 ]\n",
      "  [-7.3727603 -9.03421  ]\n",
      "  [-7.147815  -9.073313 ]\n",
      "  [-7.4311113 -8.84461  ]\n",
      "  [-6.974276  -8.604185 ]\n",
      "  [-6.8365254 -8.962904 ]\n",
      "  [-7.0885196 -9.268015 ]\n",
      "  [-6.3831367 -8.340755 ]\n",
      "  [-7.8669105 -9.225906 ]\n",
      "  [-7.2238803 -9.4149475]\n",
      "  [-7.1612797 -8.786692 ]\n",
      "  [-6.78789   -8.47434  ]\n",
      "  [-6.2442966 -8.337561 ]\n",
      "  [-6.640429  -8.48126  ]\n",
      "  [-7.5497904 -9.1149845]\n",
      "  [-6.9471865 -9.136858 ]\n",
      "  [-7.602379  -9.339161 ]\n",
      "  [-7.102343  -9.137878 ]\n",
      "  [-7.2320085 -8.718086 ]\n",
      "  [-7.3036265 -9.153996 ]\n",
      "  [-7.0491266 -8.879899 ]\n",
      "  [-7.6597543 -9.098769 ]\n",
      "  [-7.908033  -9.388782 ]\n",
      "  [-7.8969493 -9.461687 ]\n",
      "  [-7.0710254 -9.13192  ]\n",
      "  [-7.419212  -8.936222 ]\n",
      "  [-7.2402883 -8.917381 ]\n",
      "  [-6.3360095 -8.640766 ]\n",
      "  [-7.621573  -8.938055 ]\n",
      "  [-6.403578  -8.470832 ]\n",
      "  [-6.804376  -8.980831 ]\n",
      "  [-7.157377  -9.4182625]\n",
      "  [-6.0427375 -9.030027 ]\n",
      "  [-7.5543    -9.058198 ]\n",
      "  [-7.198305  -9.262334 ]\n",
      "  [-7.785827  -9.167988 ]\n",
      "  [-6.6672516 -9.439617 ]\n",
      "  [-7.596252  -8.957939 ]\n",
      "  [-7.3877234 -9.172874 ]\n",
      "  [-7.1734886 -8.823116 ]\n",
      "  [-6.351645  -8.988712 ]\n",
      "  [-7.1991496 -8.962786 ]\n",
      "  [-6.925977  -9.182544 ]\n",
      "  [-7.04277   -8.745186 ]\n",
      "  [-6.9050593 -8.847593 ]\n",
      "  [-5.255998  -8.410765 ]\n",
      "  [-7.466371  -8.982254 ]\n",
      "  [-7.4695554 -9.144313 ]\n",
      "  [-6.981162  -8.690577 ]\n",
      "  [-6.623566  -8.673454 ]\n",
      "  [-7.1805277 -9.260151 ]\n",
      "  [-7.929619  -9.371334 ]\n",
      "  [-7.315746  -8.973654 ]\n",
      "  [-6.7111335 -9.129612 ]\n",
      "  [-6.290747  -8.808704 ]\n",
      "  [-7.4255214 -9.064413 ]\n",
      "  [-6.653349  -8.697696 ]\n",
      "  [-6.9059863 -8.929831 ]\n",
      "  [-7.811538  -9.15848  ]\n",
      "  [-7.0853443 -8.950762 ]\n",
      "  [-7.5099654 -9.264015 ]\n",
      "  [-7.8731546 -9.55698  ]\n",
      "  [-7.434177  -9.016123 ]\n",
      "  [-7.364165  -8.964767 ]\n",
      "  [-7.6469827 -8.916254 ]\n",
      "  [-6.864055  -8.96937  ]\n",
      "  [-6.647043  -8.579722 ]\n",
      "  [-7.566776  -9.200348 ]\n",
      "  [-6.9534183 -8.566631 ]\n",
      "  [-6.8363643 -8.851333 ]\n",
      "  [-6.653431  -9.126951 ]\n",
      "  [-6.634934  -9.171268 ]\n",
      "  [-7.1624236 -9.48582  ]\n",
      "  [-7.6783304 -9.114157 ]\n",
      "  [-7.014727  -8.871788 ]\n",
      "  [-7.235516  -8.550867 ]\n",
      "  [-7.2323613 -9.520908 ]\n",
      "  [-7.5256305 -8.812698 ]\n",
      "  [-6.4704785 -9.030242 ]\n",
      "  [-7.626365  -9.340191 ]\n",
      "  [-7.377729  -9.002582 ]\n",
      "  [-7.209657  -8.714546 ]\n",
      "  [-6.822799  -8.720335 ]\n",
      "  [-6.928528  -9.025723 ]\n",
      "  [-6.3278766 -8.847667 ]\n",
      "  [-6.6899624 -9.178627 ]\n",
      "  [-6.763598  -8.618444 ]\n",
      "  [-7.239307  -8.991377 ]\n",
      "  [-7.1821537 -9.412586 ]\n",
      "  [-6.8983645 -8.815206 ]\n",
      "  [-6.912491  -8.629667 ]\n",
      "  [-6.9028697 -9.022295 ]\n",
      "  [-7.4601545 -8.990302 ]\n",
      "  [-6.89254   -9.003472 ]\n",
      "  [-7.6388335 -8.779097 ]\n",
      "  [-7.116277  -9.159651 ]\n",
      "  [-5.2752266 -9.228258 ]\n",
      "  [-7.1460514 -9.1019125]\n",
      "  [-7.121237  -9.066685 ]\n",
      "  [-6.598458  -8.976324 ]\n",
      "  [-5.80996   -8.866142 ]\n",
      "  [-7.405713  -9.283174 ]\n",
      "  [-7.4174438 -8.964455 ]\n",
      "  [-7.3242583 -9.184029 ]\n",
      "  [-7.3997316 -9.175342 ]\n",
      "  [-7.3029504 -9.174681 ]\n",
      "  [-6.678883  -8.8658695]\n",
      "  [-7.133364  -9.0267105]\n",
      "  [-7.2457643 -8.712463 ]\n",
      "  [-7.2984776 -9.270092 ]\n",
      "  [-6.9543476 -8.589752 ]\n",
      "  [-7.1283345 -9.286832 ]\n",
      "  [-7.2938986 -9.051897 ]\n",
      "  [-6.5191436 -8.619246 ]\n",
      "  [-7.544356  -9.137747 ]\n",
      "  [-6.8566566 -8.92761  ]\n",
      "  [-7.0349116 -8.952172 ]\n",
      "  [-6.9817996 -8.885882 ]\n",
      "  [-6.6797233 -8.670802 ]\n",
      "  [-7.168856  -9.224765 ]\n",
      "  [-7.8770156 -9.085283 ]\n",
      "  [-6.777262  -8.92664  ]\n",
      "  [-7.3057246 -9.091624 ]\n",
      "  [-7.6199107 -9.004612 ]\n",
      "  [-6.4878926 -8.955038 ]\n",
      "  [-7.685963  -9.695473 ]\n",
      "  [-7.056667  -9.047495 ]\n",
      "  [-7.4498487 -8.885418 ]\n",
      "  [-7.5042214 -9.092861 ]\n",
      "  [-7.017571  -8.61017  ]\n",
      "  [-6.697313  -8.789545 ]\n",
      "  [-6.923649  -9.255054 ]\n",
      "  [-7.083555  -8.753786 ]\n",
      "  [-7.1056724 -9.357707 ]\n",
      "  [-6.526225  -8.993252 ]\n",
      "  [-6.632381  -9.644676 ]\n",
      "  [-7.1133    -8.845985 ]\n",
      "  [-5.689808  -8.798626 ]\n",
      "  [-6.860204  -9.418663 ]\n",
      "  [-6.7327733 -8.9557   ]\n",
      "  [-6.82531   -8.930082 ]\n",
      "  [-6.478043  -8.334273 ]\n",
      "  [-7.541399  -9.066431 ]\n",
      "  [-7.500783  -8.936996 ]\n",
      "  [-7.192573  -9.053162 ]\n",
      "  [-6.5609136 -8.702325 ]\n",
      "  [-7.2680364 -9.21076  ]\n",
      "  [-6.6633286 -8.7950535]\n",
      "  [-7.4046097 -9.108093 ]\n",
      "  [-7.544047  -9.11879  ]\n",
      "  [-6.976062  -9.537714 ]\n",
      "  [-7.0578685 -8.686292 ]\n",
      "  [-6.6228175 -9.014168 ]\n",
      "  [-6.900912  -9.442857 ]\n",
      "  [-6.0498643 -8.29447  ]\n",
      "  [-7.880467  -9.028804 ]\n",
      "  [-6.7030635 -9.10052  ]\n",
      "  [-8.080761  -8.816035 ]\n",
      "  [-7.34745   -9.007272 ]\n",
      "  [-7.2618413 -9.055417 ]\n",
      "  [-7.285811  -9.233091 ]\n",
      "  [-6.7245426 -8.686248 ]\n",
      "  [-6.987412  -8.84446  ]\n",
      "  [-7.205071  -9.277082 ]\n",
      "  [-7.410957  -8.7922   ]\n",
      "  [-7.461717  -9.052532 ]\n",
      "  [-7.4056    -9.10124  ]\n",
      "  [-7.0995374 -9.52161  ]\n",
      "  [-7.4182515 -9.125185 ]\n",
      "  [-6.0656247 -8.274847 ]\n",
      "  [-7.1954246 -9.140566 ]\n",
      "  [-7.686825  -9.0842285]\n",
      "  [-7.292467  -9.400604 ]\n",
      "  [-7.847706  -9.579302 ]\n",
      "  [-7.193484  -8.925781 ]\n",
      "  [-7.33454   -9.016645 ]\n",
      "  [-7.558168  -9.12936  ]\n",
      "  [-7.570006  -9.218622 ]\n",
      "  [-5.9289594 -8.938833 ]\n",
      "  [-7.5243974 -9.307838 ]\n",
      "  [-7.696326  -9.236294 ]\n",
      "  [-7.5053797 -8.939522 ]\n",
      "  [-6.9251285 -8.385246 ]\n",
      "  [-7.5058503 -9.404534 ]\n",
      "  [-6.324017  -9.138216 ]\n",
      "  [-6.841701  -8.631677 ]\n",
      "  [-7.060745  -9.217594 ]\n",
      "  [-6.7575226 -9.283956 ]\n",
      "  [-7.4866333 -9.337472 ]\n",
      "  [-7.614824  -9.082336 ]\n",
      "  [-6.981395  -9.273642 ]\n",
      "  [-6.8585057 -9.095247 ]\n",
      "  [-7.2469745 -9.091846 ]\n",
      "  [-7.675987  -9.297335 ]\n",
      "  [-7.656546  -8.648054 ]\n",
      "  [-7.3965263 -8.94104  ]\n",
      "  [-7.5732465 -8.890698 ]\n",
      "  [-7.0419354 -9.072309 ]\n",
      "  [-7.591518  -8.673028 ]\n",
      "  [-7.6213236 -8.8929   ]\n",
      "  [-6.339427  -8.562641 ]\n",
      "  [-6.8712187 -9.123231 ]\n",
      "  [-6.180651  -9.0417795]\n",
      "  [-7.5301394 -9.150563 ]\n",
      "  [-6.271001  -9.2944145]\n",
      "  [-7.3668146 -9.350629 ]\n",
      "  [-6.3451715 -9.067049 ]\n",
      "  [-7.407197  -9.206747 ]\n",
      "  [-6.4681115 -8.828393 ]\n",
      "  [-6.428332  -8.642296 ]\n",
      "  [-6.091961  -8.267808 ]\n",
      "  [-6.1142087 -8.734119 ]\n",
      "  [-7.153417  -9.269338 ]\n",
      "  [-7.4760585 -9.15657  ]]]\n",
      "  Predicted boxes shape: torch.Size([1, 300, 4])\n",
      "  Predicted boxes: [[[0.43655825 0.12672038 0.07770786 0.06585684]\n",
      "  [0.21758968 0.3681766  0.25855762 0.15538974]\n",
      "  [0.42272502 0.1905881  0.08651044 0.08456572]\n",
      "  ...\n",
      "  [0.687561   0.2631404  0.05051888 0.04790047]\n",
      "  [0.15218228 0.5155921  0.04256795 0.06144353]\n",
      "  [0.80435586 0.54751396 0.04563199 0.07374096]]]\n",
      "  Confidence scores (pre-post-processing): [[0.9035593  0.88464254 0.82490474 0.8055158  0.8065084  0.8998173\n",
      "  0.84972507 0.9143393  0.8346379  0.82395214 0.8538108  0.8474075\n",
      "  0.8543539  0.82305706 0.85218745 0.96456236 0.850681   0.8926481\n",
      "  0.8983987  0.82451266 0.88690835 0.8290585  0.84310615 0.8172946\n",
      "  0.8675531  0.9112157  0.88881767 0.8975491  0.8403909  0.8652272\n",
      "  0.85539037 0.9159351  0.88106465 0.8819728  0.8551569  0.827196\n",
      "  0.89621276 0.90981305 0.7881322  0.8650942  0.88058865 0.829503\n",
      "  0.865298   0.80987006 0.8808882  0.7919603  0.88659394 0.8629099\n",
      "  0.85497355 0.8922633  0.8518832  0.8451264  0.82559806 0.8680198\n",
      "  0.8829611  0.896416   0.8699545  0.788845   0.8823068  0.88603234\n",
      "  0.8493738  0.8424584  0.824835   0.9130251  0.942572   0.8749415\n",
      "  0.87374216 0.8442918  0.7328688  0.89566535 0.8118729  0.8511548\n",
      "  0.8004862  0.89732385 0.83236784 0.82473224 0.8669017  0.8944495\n",
      "  0.8640989  0.90348995 0.8360001  0.8942335  0.9122674  0.8482531\n",
      "  0.8008519  0.8608554  0.8690361  0.8404326  0.8727502  0.80431724\n",
      "  0.83615714 0.8934407  0.898393   0.876275   0.7955964  0.8994445\n",
      "  0.8355402  0.8437568  0.89024675 0.863047   0.8270974  0.89931816\n",
      "  0.85027784 0.88447785 0.8154888  0.86417055 0.86185366 0.80830204\n",
      "  0.81468564 0.82703215 0.8870438  0.8200978  0.84251916 0.9092702\n",
      "  0.78859586 0.88767946 0.8981151  0.9055854  0.9519966  0.81815517\n",
      "  0.8873575  0.7993378  0.94116414 0.7960338  0.85633177 0.8388407\n",
      "  0.93320936 0.85366446 0.90521544 0.84585    0.8746303  0.9590962\n",
      "  0.81993145 0.8422091  0.84676033 0.88593626 0.8889068  0.8087201\n",
      "  0.83995694 0.9182256  0.92539114 0.83738405 0.88537514 0.88327795\n",
      "  0.79362917 0.86592716 0.8524629  0.84341043 0.82947993 0.83210266\n",
      "  0.78061795 0.8914187  0.87354565 0.83665836 0.833857   0.88235974\n",
      "  0.9222645  0.92665005 0.9107962  0.80780745 0.86495405 0.78840715\n",
      "  0.90792406 0.7836505  0.92822665 0.8473318  0.8354633  0.8183025\n",
      "  0.8696124  0.8906303  0.92551756 0.9233433  0.86469513 0.8522137\n",
      "  0.9029493  0.8717857  0.8477648  0.89277697 0.82202786 0.8919612\n",
      "  0.7577279  0.8852764  0.9811652  0.8760843  0.8749494  0.9151238\n",
      "  0.9550487  0.86731917 0.8244816  0.86527014 0.855154   0.86665833\n",
      "  0.8990748  0.86913663 0.81255513 0.8777844  0.83690864 0.8964602\n",
      "  0.85295886 0.8909131  0.83109266 0.8880478  0.8718326  0.870353\n",
      "  0.87985724 0.8865433  0.7699923  0.89561063 0.85642385 0.79974496\n",
      "  0.9218062  0.8817919  0.8798307  0.80776757 0.83042467 0.8309815\n",
      "  0.89014596 0.9114449  0.8416066  0.90482587 0.9217977  0.9531265\n",
      "  0.8497556  0.95725507 0.92813975 0.9022895  0.8913661  0.86485696\n",
      "  0.8212783  0.8078676  0.86536556 0.8948635  0.8746511  0.8939486\n",
      "  0.84598917 0.82845867 0.9283524  0.83595353 0.91616535 0.9270305\n",
      "  0.9041842  0.759207   0.9166332  0.6759616  0.84021413 0.85736513\n",
      "  0.8751498  0.8767174  0.86495245 0.88815296 0.7991906  0.8307308\n",
      "  0.8449645  0.91849506 0.8464381  0.9010746  0.8749159  0.8017716\n",
      "  0.89169157 0.8496164  0.849706   0.8431831  0.8279535  0.838704\n",
      "  0.9530182  0.85612124 0.82346004 0.80754584 0.8115507  0.8697425\n",
      "  0.9434383  0.85692424 0.8963071  0.92597425 0.86422557 0.81267905\n",
      "  0.9082329  0.9035008  0.8635239  0.83498096 0.7293857  0.8241199\n",
      "  0.7887575  0.8839494  0.7467797  0.7810126  0.90231484 0.904824\n",
      "  0.94589114 0.8348536  0.9536208  0.87908715 0.9383053  0.8580941\n",
      "  0.913748   0.9014965  0.89805955 0.93213207 0.892441   0.84297234]]\n",
      "Post-processed results:\n",
      "  Boxes: []\n",
      "  Labels: []\n",
      "  Scores: []\n",
      "Warning: No detections for /home/ai-user/Paligemma/bbox/newthous/Normal/2025_03_12_24440223_7A1E03C6_EEEF769F.jpeg with confidence >= 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Color' has no attribute 'red'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnnotated images saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Process images\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path, gt_label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(image_paths, gt_labels), total=\u001b[38;5;28mlen\u001b[39m(image_paths), desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing images\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     prediction, confidence = \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     results.append({\n\u001b[32m    201\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimage_name\u001b[39m\u001b[33m'\u001b[39m: img_path,\n\u001b[32m    202\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m'\u001b[39m: gt_label,\n\u001b[32m    203\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m: prediction,\n\u001b[32m    204\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m: confidence\n\u001b[32m    205\u001b[39m     })\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Save results to CSV\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mvisualize_predictions\u001b[39m\u001b[34m(image_path, gt_label, model, processor, output_dir, id2label)\u001b[39m\n\u001b[32m     95\u001b[39m annotated_image_np = np.array(pil_image)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Use BoxAnnotator with minimal configuration for compatibility\u001b[39;00m\n\u001b[32m     97\u001b[39m box_annotator = sv.BoxAnnotator(\n\u001b[32m     98\u001b[39m     thickness=\u001b[32m2\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     color=\u001b[43msv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mColor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mred\u001b[49m()  \u001b[38;5;66;03m# Use red for visibility\u001b[39;00m\n\u001b[32m    100\u001b[39m )\n\u001b[32m    101\u001b[39m annotated_image_np = box_annotator.annotate(\n\u001b[32m    102\u001b[39m     scene=annotated_image_np,\n\u001b[32m    103\u001b[39m     detections=detections\n\u001b[32m    104\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Use LabelAnnotator for labels\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'Color' has no attribute 'red'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "import pandas as pd\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "# Define constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"/home/ai-user/Paligemma/detr/RT_DeTr/runs/final\"\n",
    "DATA_DIR = \"/home/ai-user/Paligemma/bbox/newthous\"\n",
    "OUTPUT_DIR = \"/home/ai-user/Paligemma/detr/RT_DeTr/inference_results\"\n",
    "OUTPUT_CSV_PATH = \"/home/ai-user/Paligemma/detr/RT_DeTr/inference_results/predictions_rtdetr.csv\"\n",
    "CLASS_NAMES = ['Normal', 'Fracture']  # Matches subdirectory names\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "CONFIDENCE_THRESHOLD = 0.05  # Further lowered to allow more detections\n",
    "\n",
    "def load_and_preprocess_image(image_path, processor):\n",
    "    \"\"\"Load and preprocess an image for RT-DETR inference.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        # Convert BGR to RGB if needed\n",
    "        if image_np.shape[2] == 3:\n",
    "            image_np = image_np[:, :, ::-1]\n",
    "        # Process image with the image processor\n",
    "        inputs = processor(images=image_np, return_tensors=\"pt\")\n",
    "        return image, inputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def visualize_predictions(image_path, gt_label, model, processor, output_dir, id2label):\n",
    "    \"\"\"Run inference, visualize predictions, and save annotated image.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pil_image, inputs = load_and_preprocess_image(image_path, processor)\n",
    "    if pil_image is None or inputs is None:\n",
    "        print(f\"Skipping {image_path} due to loading error\")\n",
    "        return \"No prediction\", \"No confidence\"\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Print raw model outputs\n",
    "    print(f\"\\nProcessing image: {image_path}\")\n",
    "    print(f\"Raw model outputs:\")\n",
    "    print(f\"  Logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"  Logits: {outputs.logits.cpu().numpy()}\")\n",
    "    print(f\"  Predicted boxes shape: {outputs.pred_boxes.shape}\")\n",
    "    print(f\"  Predicted boxes: {outputs.pred_boxes.cpu().numpy()}\")\n",
    "    \n",
    "    # Compute confidence scores from logits\n",
    "    scores = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0].cpu().numpy()\n",
    "    print(f\"  Confidence scores (pre-post-processing): {scores}\")\n",
    "    \n",
    "    # Post-process predictions\n",
    "    target_sizes = torch.tensor([pil_image.size[::-1]]).to(DEVICE)  # [height, width]\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, threshold=CONFIDENCE_THRESHOLD, target_sizes=target_sizes\n",
    "    )[0]\n",
    "    \n",
    "    # Print post-processed results\n",
    "    print(f\"Post-processed results:\")\n",
    "    print(f\"  Boxes: {results['boxes'].cpu().numpy()}\")\n",
    "    print(f\"  Labels: {results['labels'].cpu().numpy()}\")\n",
    "    print(f\"  Scores: {results['scores'].cpu().numpy()}\")\n",
    "    \n",
    "    # Convert to supervision Detections\n",
    "    detections = sv.Detections(\n",
    "        xyxy=results['boxes'].cpu().numpy(),\n",
    "        class_id=results['labels'].cpu().numpy(),\n",
    "        confidence=results['scores'].cpu().numpy()\n",
    "    )\n",
    "    \n",
    "    # Check if detections are empty\n",
    "    if len(detections.xyxy) == 0:\n",
    "        print(f\"Warning: No detections for {image_path} with confidence >= {CONFIDENCE_THRESHOLD}\")\n",
    "    \n",
    "    # Annotate image with bounding boxes\n",
    "    labels = [\n",
    "        f\"{id2label[class_id]} {confidence:.2f}\"\n",
    "        for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "    ]\n",
    "    \n",
    "    annotated_image_np = np.array(pil_image)\n",
    "    # Use BoxAnnotator with minimal configuration for compatibility\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2,\n",
    "        color=sv.Color.red()  # Use red for visibility\n",
    "    )\n",
    "    annotated_image_np = box_annotator.annotate(\n",
    "        scene=annotated_image_np,\n",
    "        detections=detections\n",
    "    )\n",
    "    \n",
    "    # Use LabelAnnotator for labels\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        color=sv.Color.red()\n",
    "    )\n",
    "    annotated_image_np = label_annotator.annotate(\n",
    "        scene=annotated_image_np,\n",
    "        detections=detections,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    # Add ground truth label to image\n",
    "    cv2.putText(\n",
    "        annotated_image_np,\n",
    "        f\"Ground Truth: {gt_label}\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1.5,\n",
    "        (0, 0, 255),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    \n",
    "    # Save annotated image\n",
    "    filename = os.path.basename(image_path)\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(annotated_image_np, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Saved annotated image to {save_path}\")\n",
    "    \n",
    "    # Prepare prediction results\n",
    "    pred_labels = [id2label[class_id] for class_id in detections.class_id]\n",
    "    confidences = [f\"{conf:.2f}\" for conf in detections.confidence]\n",
    "    \n",
    "    return (\n",
    "        \"; \".join(pred_labels) if pred_labels else \"No prediction\",\n",
    "        \"; \".join(confidences) if confidences else \"No confidence\"\n",
    "    )\n",
    "\n",
    "def get_image_paths_and_labels(data_dir):\n",
    "    \"\"\"Load image paths and their corresponding ground truth labels from subdirectories.\"\"\"\n",
    "    image_paths = []\n",
    "    gt_labels = []\n",
    "    \n",
    "    for class_name in CLASS_NAMES:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Warning: Directory {class_dir} does not exist. Skipping...\")\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(class_dir, img_name))\n",
    "                gt_labels.append(class_name)\n",
    "    \n",
    "    # Shuffle images and labels together\n",
    "    combined = list(zip(image_paths, gt_labels))\n",
    "    np.random.shuffle(combined)\n",
    "    image_paths, gt_labels = zip(*combined) if combined else ([], [])\n",
    "    \n",
    "    return list(image_paths), list(gt_labels)\n",
    "\n",
    "def main():\n",
    "    # Load image processor and model\n",
    "    print(\"Loading RT-DETR model and processor...\")\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForObjectDetection.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            num_labels=NUM_CLASSES,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or processor: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Define id2label mapping (consistent with training)\n",
    "    id2label = {i: CLASS_NAMES[i] for i in range(NUM_CLASSES)}\n",
    "    \n",
    "    # Load image paths and labels from directory\n",
    "    print(f\"Loading images from {DATA_DIR}...\")\n",
    "    image_paths, gt_labels = get_image_paths_and_labels(DATA_DIR)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"No images found in {DATA_DIR}. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images for inference.\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process images\n",
    "    for img_path, gt_label in tqdm(zip(image_paths, gt_labels), total=len(image_paths), desc=\"Processing images\"):\n",
    "        prediction, confidence = visualize_predictions(\n",
    "            img_path, gt_label, model, processor, OUTPUT_DIR, id2label\n",
    "        )\n",
    "        results.append({\n",
    "            'image_name': img_path,\n",
    "            'ground_truth': gt_label,\n",
    "            'predictions': prediction,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV_PATH), exist_ok=True)\n",
    "    results_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"Predictions saved to {OUTPUT_CSV_PATH}\")\n",
    "    print(f\"Annotated images saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "380d9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from roboflow import Roboflow\n",
    "from dataclasses import dataclass, replace\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2386e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load model\n",
    "\n",
    "CHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "processor = AutoImageProcessor.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b8a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Path to the local image file\n",
    "image_path = \"/home/ai-user/Paligemma/detr/RT_DeTr/inference_results/2025_05_03_FEDF1F44_3BFEE365_1F51FDB0.jpeg\"\n",
    "\n",
    "# Open the image from the local file path\n",
    "image = Image.open(image_path)\n",
    "inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get image dimensions\n",
    "w, h = image.size\n",
    "# Post-process the results\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, target_sizes=[(h, w)], threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "629cd54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated image saved to /home/ai-user/Paligemma/detr/RT_DeTr/inference_results/annotated_image.jpeg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import supervision as sv\n",
    "\n",
    "# Path to the local image file\n",
    "image_path = \"/home/ai-user/Paligemma/detr/RT_DeTr/inference_results/2025_05_03_FEDF1F44_3BFEE365_1F51FDB0.jpeg\"\n",
    "\n",
    "# Open the image from the local file path\n",
    "image = Image.open(image_path)\n",
    "inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get image dimensions\n",
    "w, h = image.size\n",
    "# Post-process the results\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, target_sizes=[(h, w)], threshold=0.3\n",
    ")\n",
    "\n",
    "# Convert transformer results to supervision Detections\n",
    "detections = sv.Detections.from_transformers(results[0])\n",
    "\n",
    "# Extract labels from class IDs\n",
    "labels = [\n",
    "    model.config.id2label[class_id]\n",
    "    for class_id in detections.class_id\n",
    "]\n",
    "\n",
    "# Annotate the image\n",
    "annotated_image = image.copy()\n",
    "annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\n",
    "annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels=labels)\n",
    "\n",
    "# Resize the annotated image to a thumbnail (max 600x600)\n",
    "annotated_image.thumbnail((600, 600))\n",
    "\n",
    "# Save the annotated image to a file\n",
    "output_path = \"/home/ai-user/Paligemma/detr/RT_DeTr/inference_results/annotated_image.jpeg\"\n",
    "annotated_image.save(output_path)\n",
    "print(f\"Annotated image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b4637",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# @title Download dataset from Roboflow Universe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ROBOFLOW_API_KEY = \u001b[43muserdata\u001b[49m.get(\u001b[33m'\u001b[39m\u001b[33mROBOFLOW_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\u001b[32m      6\u001b[39m project = rf.workspace(\u001b[33m\"\u001b[39m\u001b[33mroboflow-jvuqo\u001b[39m\u001b[33m\"\u001b[39m).project(\u001b[33m\"\u001b[39m\u001b[33mpoker-cards-fmjio\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'userdata' is not defined"
     ]
    }
   ],
   "source": [
    "ds_train = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/train\",\n",
    "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
    ")\n",
    "ds_valid = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/valid\",\n",
    "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
    ")\n",
    "ds_test = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/test\",\n",
    "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "print(f\"Number of training images: {len(ds_train)}\")\n",
    "print(f\"Number of validation images: {len(ds_valid)}\")\n",
    "print(f\"Number of test images: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cba87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
